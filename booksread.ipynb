{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "pd.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "DIR_GENRE = './genre/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6a861a8b3f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spoilers(file_name):\n",
    "    print('counting file:', file_name)\n",
    "    n_review, n_sentence, n_spoiler_review, n_spoiler_sentence = 0, 0, 0, 0\n",
    "    book_set, user_set = set(), set()\n",
    "    print('current line: ', end='')\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            if n_review % 1000000 == 0:\n",
    "                print(n_review, end=',')\n",
    "            n_review += 1\n",
    "            for _t, _ in d['review_sentences']:\n",
    "                n_sentence += 1\n",
    "                n_spoiler_sentence += _t\n",
    "            n_spoiler_review += int(d['has_spoiler'])\n",
    "            book_set.add(d['book_id'])\n",
    "            user_set.add(d['user_id'])\n",
    "    print('complete')\n",
    "    print('done!')\n",
    "    return n_review, n_sentence, n_spoiler_review, n_spoiler_sentence, len(book_set), len(user_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting file: ./books/goodreads_reviews_spoiler.json.gz\n",
      "current line: 0,1000000,complete\n",
      "done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># review</th>\n",
       "      <td>1,378,033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># sentence</th>\n",
       "      <td>17,672,655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># spoiler review</th>\n",
       "      <td>89,627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># spoiler sentence</th>\n",
       "      <td>569,724.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># book</th>\n",
       "      <td>25,475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># user</th>\n",
       "      <td>18,892.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "# review            1,378,033.0\n",
       "# sentence         17,672,655.0\n",
       "# spoiler review       89,627.0\n",
       "# spoiler sentence    569,724.0\n",
       "# book                 25,475.0\n",
       "# user                 18,892.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = count_spoilers(os.path.join(DIR, 'books/goodreads_reviews_spoiler.json.gz'))\n",
    "df_stats_spoiler = pd.DataFrame(res, columns=['count'], dtype=float, \n",
    "                               index=['# review', '# sentence', '# spoiler review', '# spoiler sentence',\n",
    "                                      '# book', '# user'])\n",
    "display(df_stats_spoiler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#        \n",
    "#             \n",
    "# Code From Here\n",
    "#        \n",
    "#        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, head = 100000):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "#             print(count)\n",
    "            data.append(d)\n",
    "            \n",
    "            # break if reaches the 100th line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews = load_data(os.path.join(DIR, 'books/books_5.json.gz'))\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6a861a8b3f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 4.0,\n",
       " 'vote': '2',\n",
       " 'verified': False,\n",
       " 'reviewTime': '07 31, 2003',\n",
       " 'reviewerID': 'A11U25L0QSIA5E',\n",
       " 'asin': '0140106995',\n",
       " 'style': {'Format:': ' Paperback'},\n",
       " 'reviewerName': 'Charlie Watanabe',\n",
       " 'reviewText': \"I'm a sucker for books that chronicle a man's life.  I found it very enjoyable as J.J. Todd's life moves in parallel to his obsession, Roussou's CONFESSIONS.\\nThis book reminded me of Mark Helprin's SOLDIER OF THE GREAT WAR.\",\n",
       " 'summary': \"An Old Man's Story\",\n",
       " 'unixReviewTime': 1059609600}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(reviews)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# label = []\n",
    "review = []\n",
    "summary = []\n",
    "# asin_r = []\n",
    "for i in range(500000,len(reviews)):\n",
    "#     asin_r.append(reviews[i]['asin'])\n",
    "#     label.append(reviews[i]['overall'])\n",
    "    try:\n",
    "        review.append(reviews[i]['reviewText'])\n",
    "    except:\n",
    "        review.append(\"\")\n",
    "    try:\n",
    "        summary.append(reviews[i]['summary'])\n",
    "    except:\n",
    "        summary.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The King, the Mice and the Cheese by Nancy Gur...</td>\n",
       "      <td>A story children will love and learn from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The kids loved it!</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My students (3 &amp; 4 year olds) loved this book!...</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOVE IT</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499996</th>\n",
       "      <td>I'm a great admirer of Willian Boyd, a person ...</td>\n",
       "      <td>a very different opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499997</th>\n",
       "      <td>A wonderful rampage through the twentieth cent...</td>\n",
       "      <td>Shades of Tristram Shandy (Stern) and Tom Jone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499998</th>\n",
       "      <td>William Boyd is a terrific storyteller.  His p...</td>\n",
       "      <td>another sweeping saga by Boyd fully entertains..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499999</th>\n",
       "      <td>This fictional memoir displays Boyd's consumma...</td>\n",
       "      <td>An Outstanding Fictional Memoir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500000</th>\n",
       "      <td>I'm a sucker for books that chronicle a man's ...</td>\n",
       "      <td>An Old Man's Story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000001 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  \\\n",
       "0        The King, the Mice and the Cheese by Nancy Gur...   \n",
       "1                                       The kids loved it!   \n",
       "2        My students (3 & 4 year olds) loved this book!...   \n",
       "3                                                  LOVE IT   \n",
       "4                                                   Great!   \n",
       "...                                                    ...   \n",
       "1499996  I'm a great admirer of Willian Boyd, a person ...   \n",
       "1499997  A wonderful rampage through the twentieth cent...   \n",
       "1499998  William Boyd is a terrific storyteller.  His p...   \n",
       "1499999  This fictional memoir displays Boyd's consumma...   \n",
       "1500000  I'm a sucker for books that chronicle a man's ...   \n",
       "\n",
       "                                                   summary  \n",
       "0                A story children will love and learn from  \n",
       "1                                               Five Stars  \n",
       "2                                               Five Stars  \n",
       "3                                               Five Stars  \n",
       "4                                               Five Stars  \n",
       "...                                                    ...  \n",
       "1499996                           a very different opinion  \n",
       "1499997  Shades of Tristram Shandy (Stern) and Tom Jone...  \n",
       "1499998   another sweeping saga by Boyd fully entertains..  \n",
       "1499999                    An Outstanding Fictional Memoir  \n",
       "1500000                                 An Old Man's Story  \n",
       "\n",
       "[2000001 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(list(zip(review, summary)), columns =['review', 'summary'])\n",
    "# df1.set_index('asin',inplace=True)\n",
    "# df1[\"title\"] = \"\"\n",
    "df1 = pd.concat([df1,df2])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"books/twenty_lakh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a861a8b3f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80982, 4), (95333, 4), (823686, 4))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "below_3 = df1[df1['label']<3].copy\n",
    "equal_3 = df1[df1['label']==3].copy\n",
    "above_3 = df1[df1['label']>3].copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = load_data(os.path.join(DIR, 'books/meta_Books.json.gz'))\n",
    "len(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[0]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abuse',\n",
       " 'Accounting',\n",
       " 'Action & Adventure',\n",
       " 'Action &amp; Adventure',\n",
       " 'Activities, Crafts & Games',\n",
       " 'Activities, Crafts &amp; Games',\n",
       " 'Addiction & Recovery',\n",
       " 'Addiction &amp; Recovery',\n",
       " 'Administration & Medicine Economics',\n",
       " 'Administration &amp; Medicine Economics',\n",
       " 'Administrative Law',\n",
       " 'Adoption',\n",
       " 'Africa',\n",
       " 'Aging',\n",
       " 'Aging Parents',\n",
       " 'Agricultural Sciences',\n",
       " 'Allied Health Professions',\n",
       " 'Almanacs & Yearbooks',\n",
       " 'Almanacs &amp; Yearbooks',\n",
       " 'Alternative Medicine',\n",
       " 'Americas',\n",
       " 'Ancient & Medieval Literature',\n",
       " 'Ancient &amp; Medieval Literature',\n",
       " 'Ancient Civilizations',\n",
       " 'Anger Management',\n",
       " 'Animals',\n",
       " 'Anthologies',\n",
       " 'Anthropology',\n",
       " 'Antiques & Collectibles',\n",
       " 'Antiques &amp; Collectibles',\n",
       " 'Anxieties & Phobias',\n",
       " 'Anxieties &amp; Phobias',\n",
       " 'Archaeology',\n",
       " 'Architecture',\n",
       " 'Art, Music & Photography',\n",
       " 'Art, Music &amp; Photography',\n",
       " 'Arts',\n",
       " 'Arts & Literature',\n",
       " 'Arts & Photography',\n",
       " 'Arts &amp; Literature',\n",
       " 'Arts &amp; Photography',\n",
       " 'Arts, Music & Photography',\n",
       " 'Arts, Music &amp; Photography',\n",
       " 'Asia',\n",
       " 'Astronomy & Space Science',\n",
       " 'Astronomy &amp; Space Science',\n",
       " 'Atheism',\n",
       " 'Atlases & Maps',\n",
       " 'Atlases &amp; Maps',\n",
       " 'Australia & Oceania',\n",
       " 'Australia & South Pacific',\n",
       " 'Australia &amp; Oceania',\n",
       " 'Australia &amp; South Pacific',\n",
       " 'Automotive',\n",
       " 'Aviation',\n",
       " 'Baking',\n",
       " 'Baseball',\n",
       " 'Basic Sciences',\n",
       " 'Basketball',\n",
       " 'Beauty, Grooming, & Style',\n",
       " 'Beauty, Grooming, &amp; Style',\n",
       " 'Behavioral Psychology',\n",
       " 'Behavioral Sciences',\n",
       " 'Beverages & Wine',\n",
       " 'Beverages &amp; Wine',\n",
       " 'Bible Study & Reference',\n",
       " 'Bible Study &amp; Reference',\n",
       " 'Bibles',\n",
       " 'Biographies',\n",
       " 'Biographies & Memoirs',\n",
       " 'Biographies &amp; Memoirs',\n",
       " 'Biography & History',\n",
       " 'Biography &amp; History',\n",
       " 'Biological Sciences',\n",
       " 'Books',\n",
       " 'Books & Reading',\n",
       " 'British & Irish',\n",
       " 'British &amp; Irish',\n",
       " 'Buddhism',\n",
       " 'Business',\n",
       " 'Business & Finance',\n",
       " 'Business & Money',\n",
       " 'Business &amp; Finance',\n",
       " 'Business &amp; Money',\n",
       " 'Business Culture',\n",
       " 'Business Technology',\n",
       " 'Business of Art',\n",
       " 'Calculus',\n",
       " 'Calendars',\n",
       " 'Canada',\n",
       " 'Canning & Preserving',\n",
       " 'Canning &amp; Preserving',\n",
       " 'Caribbean',\n",
       " 'Cars, Trains & Things That Go',\n",
       " 'Cars, Trains &amp; Things That Go',\n",
       " 'Catalogs & Directories',\n",
       " 'Catalogs &amp; Directories',\n",
       " 'Catholicism',\n",
       " 'Celebrities & TV Shows',\n",
       " 'Central America',\n",
       " 'Certification',\n",
       " 'Certification &amp; Development',\n",
       " 'Chemistry',\n",
       " \"Children's\",\n",
       " \"Children's Books\",\n",
       " \"Children's Cookbooks\",\n",
       " \"Children's Health\",\n",
       " 'Christian Books & Bibles',\n",
       " 'Christian Books &amp; Bibles',\n",
       " 'Christian Denominations & Sects',\n",
       " 'Christian Denominations &amp; Sects',\n",
       " 'Christian Living',\n",
       " 'Churches & Church Leadership',\n",
       " 'Churches &amp; Church Leadership',\n",
       " 'Classics',\n",
       " 'Coaching',\n",
       " 'Collections, Catalogs & Exhibitions',\n",
       " 'Collections, Catalogs &amp; Exhibitions',\n",
       " 'College & High School',\n",
       " 'College &amp; High School',\n",
       " 'Comic Strips',\n",
       " 'Comics & Graphic Novels',\n",
       " 'Comics &amp; Graphic Novels',\n",
       " 'Communication &amp; Journalism',\n",
       " 'Computer Science',\n",
       " 'Computers & Technology',\n",
       " 'Computers &amp; Technology',\n",
       " 'Constitutional Law',\n",
       " 'Consumer Guides',\n",
       " 'Contemporary',\n",
       " 'Cookbooks, Food & Wine',\n",
       " 'Cookbooks, Food &amp; Wine',\n",
       " 'Cooking Education & Reference',\n",
       " 'Cooking Education &amp; Reference',\n",
       " 'Cooking Methods',\n",
       " 'Cooking by Ingredient',\n",
       " 'Crafts & Hobbies',\n",
       " 'Crafts &amp; Hobbies',\n",
       " 'Crafts, Hobbies & Home',\n",
       " 'Crafts, Hobbies &amp; Home',\n",
       " 'Creativity',\n",
       " 'Criminal Law',\n",
       " 'Databases & Big Data',\n",
       " 'Databases &amp; Big Data',\n",
       " 'Death & Grief',\n",
       " 'Death &amp; Grief',\n",
       " 'Decorative Arts & Design',\n",
       " 'Decorative Arts &amp; Design',\n",
       " 'Dentistry',\n",
       " 'Desserts',\n",
       " 'Dictionaries & Thesauruses',\n",
       " 'Dictionaries &amp; Thesauruses',\n",
       " 'Diets & Weight Loss',\n",
       " 'Diets &amp; Weight Loss',\n",
       " 'Digital Audio, Video & Photography',\n",
       " 'Diseases & Physical Ailments',\n",
       " 'Diseases &amp; Physical Ailments',\n",
       " 'Dramas & Plays',\n",
       " 'Dramas &amp; Plays',\n",
       " 'Drawing',\n",
       " 'Dreams',\n",
       " 'Early Learning',\n",
       " 'Earth Sciences',\n",
       " 'Eating Disorders',\n",
       " 'Economics',\n",
       " 'Education',\n",
       " 'Education & Reference',\n",
       " 'Education & Teaching',\n",
       " 'Education &amp; Reference',\n",
       " 'Education &amp; Teaching',\n",
       " 'Education Theory',\n",
       " 'Educational Psychology',\n",
       " 'Emotions',\n",
       " 'Encyclopedias & Subject Guides',\n",
       " 'Encyclopedias &amp; Subject Guides',\n",
       " 'Engineering',\n",
       " 'Engineering & Transportation',\n",
       " 'Engineering &amp; Transportation',\n",
       " 'English as a Second Language',\n",
       " 'Entertaining & Holidays',\n",
       " 'Entertaining &amp; Holidays',\n",
       " 'Environment',\n",
       " 'Environmental &amp; Natural Resources Law',\n",
       " 'Erotica',\n",
       " 'Essays & Commentary',\n",
       " 'Essays & Correspondence',\n",
       " 'Essays &amp; Commentary',\n",
       " 'Essays &amp; Correspondence',\n",
       " 'Estate Planning',\n",
       " 'Ethnic & National',\n",
       " 'Ethnic &amp; National',\n",
       " 'Etiquette',\n",
       " 'Europe',\n",
       " 'Evolution',\n",
       " 'Exercise & Fitness',\n",
       " 'Exercise &amp; Fitness',\n",
       " 'Experiments, Instruments & Measurement',\n",
       " 'Experiments, Instruments &amp; Measurement',\n",
       " 'Extreme Sports',\n",
       " 'Fairy Tales',\n",
       " 'Fairy Tales, Folk Tales & Myths',\n",
       " 'Fairy Tales, Folk Tales &amp; Myths',\n",
       " 'Family Activities',\n",
       " 'Family Health',\n",
       " 'Family Law',\n",
       " 'Family Relationships',\n",
       " 'Fantasy',\n",
       " 'Fashion',\n",
       " 'Fertility',\n",
       " 'Field Guides',\n",
       " 'Finance',\n",
       " 'Food, Lodging & Transportation',\n",
       " 'Food, Lodging &amp; Transportation',\n",
       " 'Football (American)',\n",
       " 'Foreign & International Law',\n",
       " 'Foreign &amp; International Law',\n",
       " 'Foreign Language Study & Reference',\n",
       " 'Foreign Language Study &amp; Reference',\n",
       " 'Games & Strategy Guides',\n",
       " 'Games &amp; Strategy Guides',\n",
       " 'Gaming',\n",
       " 'Gardening & Landscape Design',\n",
       " 'Gardening &amp; Landscape Design',\n",
       " 'Genealogy',\n",
       " 'General',\n",
       " 'General &amp; Reference',\n",
       " 'Genre Fiction',\n",
       " 'Geography & Cultures',\n",
       " 'Geography &amp; Cultures',\n",
       " 'Golf',\n",
       " 'Gothic',\n",
       " 'Graduate School',\n",
       " 'Graphic Design',\n",
       " 'Graphic Novels',\n",
       " 'Graphics & Design',\n",
       " 'Graphics &amp; Design',\n",
       " 'Growing Up & Facts of Life',\n",
       " 'Growing Up &amp; Facts of Life',\n",
       " 'Happiness',\n",
       " 'Hardware & DIY',\n",
       " 'Hardware &amp; DIY',\n",
       " 'Health & Medical Law',\n",
       " 'Health, Fitness & Dieting',\n",
       " 'Health, Fitness &amp; Dieting',\n",
       " 'Higher & Continuing Education',\n",
       " 'Higher &amp; Continuing Education',\n",
       " 'Hiking & Camping',\n",
       " 'Hiking &amp; Camping',\n",
       " 'Hinduism',\n",
       " 'Historical',\n",
       " 'Historical Fiction',\n",
       " 'Historical Study & Educational Resources',\n",
       " 'Historical Study &amp; Educational Resources',\n",
       " 'History',\n",
       " 'History & Criticism',\n",
       " 'History & Culture',\n",
       " 'History & Philosophy',\n",
       " 'History &amp; Criticism',\n",
       " 'History &amp; Culture',\n",
       " 'History &amp; Philosophy',\n",
       " 'Hobbies &amp; Games',\n",
       " 'Hockey',\n",
       " 'Holidays & Celebrations',\n",
       " 'Holidays &amp; Celebrations',\n",
       " 'Home Improvement & Design',\n",
       " 'Home Improvement &amp; Design',\n",
       " 'How To Create Comics & Manga',\n",
       " 'How To Create Comics &amp; Manga',\n",
       " 'Human Resources',\n",
       " 'Humanities',\n",
       " 'Humor',\n",
       " 'Humor & Entertainment',\n",
       " 'Humor & Satire',\n",
       " 'Humor &amp; Entertainment',\n",
       " 'Humor &amp; Satire',\n",
       " 'Hunting & Fishing',\n",
       " 'Hunting &amp; Fishing',\n",
       " 'Hypnosis',\n",
       " 'Individual Artists',\n",
       " 'Individual Sports',\n",
       " 'Industries',\n",
       " 'Insurance',\n",
       " 'Intellectual Property',\n",
       " 'International',\n",
       " 'Internet & Social Media',\n",
       " 'Internet &amp; Social Media',\n",
       " 'Investing',\n",
       " 'Islam',\n",
       " 'Job Hunting & Careers',\n",
       " 'Job Hunting &amp; Careers',\n",
       " 'Journal Writing',\n",
       " 'Judaism',\n",
       " 'Kitchen Appliances',\n",
       " 'Law',\n",
       " 'Law Practice',\n",
       " 'Leaders & Notable People',\n",
       " 'Leaders &amp; Notable People',\n",
       " 'Legal Education',\n",
       " 'Legal History',\n",
       " 'Legal Theory & Systems',\n",
       " 'Legal Theory &amp; Systems',\n",
       " 'Lesbian, Gay, Bisexual & Transgender Books',\n",
       " 'Lesbian, Gay, Bisexual &amp; Transgender Books',\n",
       " 'Literary',\n",
       " 'Literature & Fiction',\n",
       " 'Literature &amp; Fiction',\n",
       " 'Main Courses & Side Dishes',\n",
       " 'Main Courses &amp; Side Dishes',\n",
       " 'Management & Leadership',\n",
       " 'Management &amp; Leadership',\n",
       " 'Manga',\n",
       " 'Marketing & Sales',\n",
       " 'Marketing &amp; Sales',\n",
       " 'Mathematics',\n",
       " 'Medical Books',\n",
       " 'Medicine',\n",
       " 'Medicine & Health Sciences',\n",
       " 'Medicine &amp; Health Sciences',\n",
       " 'Memoirs',\n",
       " 'Memory Improvement',\n",
       " \"Men's Health\",\n",
       " 'Mental Health',\n",
       " 'Mexico',\n",
       " 'Mid-Life',\n",
       " 'Middle East',\n",
       " 'Military',\n",
       " 'Ministry & Evangelism',\n",
       " 'Ministry &amp; Evangelism',\n",
       " 'Miscellaneous',\n",
       " 'Mobile Phones, Tablets & E-Readers',\n",
       " 'Mobile Phones, Tablets &amp; E-Readers',\n",
       " 'Motivational',\n",
       " 'Mountaineering',\n",
       " 'Movies',\n",
       " 'Music',\n",
       " 'Mysteries & Detectives',\n",
       " 'Mysteries & Thrillers',\n",
       " 'Mysteries &amp; Detectives',\n",
       " 'Mysteries &amp; Thrillers',\n",
       " 'Mystery',\n",
       " 'Mystery &amp; Thrillers',\n",
       " 'Mystery, Thriller & Suspense',\n",
       " 'Mystery, Thriller &amp; Suspense',\n",
       " 'Mythology & Folk Tales',\n",
       " 'Mythology &amp; Folk Tales',\n",
       " 'Nature & Ecology',\n",
       " 'Nature &amp; Ecology',\n",
       " 'Networking & Cloud Computing',\n",
       " 'Networking &amp; Cloud Computing',\n",
       " 'New Age',\n",
       " 'New Age & Spirituality',\n",
       " 'New Age &amp; Spirituality',\n",
       " 'New England',\n",
       " 'New, Used & Rental Textbooks',\n",
       " 'New, Used &amp; Rental Textbooks',\n",
       " 'Northeast',\n",
       " 'Nursing',\n",
       " 'Nutrition',\n",
       " 'Occult & Paranormal',\n",
       " 'Occult &amp; Paranormal',\n",
       " 'Operating Systems',\n",
       " 'Other Eastern Religions & Sacred Texts',\n",
       " 'Other Eastern Religions &amp; Sacred Texts',\n",
       " 'Other Media',\n",
       " 'Other Religions, Practices & Sacred Texts',\n",
       " 'Other Religions, Practices &amp; Sacred Texts',\n",
       " 'Other Team Sports',\n",
       " 'Outdoor Cooking',\n",
       " 'Outdoor Recreation',\n",
       " 'Painting',\n",
       " 'Paranormal',\n",
       " 'Parenting',\n",
       " 'Parenting & Relationships',\n",
       " 'Parenting &amp; Relationships',\n",
       " 'Performing Arts',\n",
       " 'Personal Finance',\n",
       " 'Personal Health',\n",
       " 'Personal Transformation',\n",
       " 'Pets & Animal Care',\n",
       " 'Pets &amp; Animal Care',\n",
       " 'Pharmacology',\n",
       " 'Philosophy',\n",
       " 'Photography & Video',\n",
       " 'Photography &amp; Video',\n",
       " 'Physics',\n",
       " 'Pictorial',\n",
       " 'Poetry',\n",
       " 'Polar Regions',\n",
       " 'Police Procedurals',\n",
       " 'Politics & Government',\n",
       " 'Politics & Social Sciences',\n",
       " 'Politics &amp; Government',\n",
       " 'Politics &amp; Social Sciences',\n",
       " 'Pop Culture',\n",
       " 'Processes & Infrastructure',\n",
       " 'Processes &amp; Infrastructure',\n",
       " 'Professional',\n",
       " 'Professional Cooking',\n",
       " 'Professionals & Academics',\n",
       " 'Professionals &amp; Academics',\n",
       " 'Programming',\n",
       " 'Programming Languages',\n",
       " 'Protestantism',\n",
       " 'Psychology',\n",
       " 'Psychology & Counseling',\n",
       " 'Psychology &amp; Counseling',\n",
       " 'Pure Mathematics',\n",
       " 'Puzzles & Games',\n",
       " 'Puzzles &amp; Games',\n",
       " 'Quick & Easy',\n",
       " 'Quick &amp; Easy',\n",
       " 'Quotations',\n",
       " 'Racket Sports',\n",
       " 'Radio',\n",
       " 'Real Estate',\n",
       " 'Reference',\n",
       " 'Reference & Collections',\n",
       " 'Reference &amp; Collections',\n",
       " 'Regency',\n",
       " 'Regional & International',\n",
       " 'Regional &amp; International',\n",
       " 'Regional Canada',\n",
       " 'Regional U.S.',\n",
       " 'Relationships',\n",
       " 'Religion & Spirituality',\n",
       " 'Religion &amp; Spirituality',\n",
       " 'Religions',\n",
       " 'Religious',\n",
       " 'Religious Studies',\n",
       " 'Research',\n",
       " 'Romance',\n",
       " 'Romantic Comedy',\n",
       " 'Romantic Suspense',\n",
       " 'Rules & Procedures',\n",
       " 'Rules &amp; Procedures',\n",
       " 'Russia',\n",
       " 'Safety & First Aid',\n",
       " 'Safety &amp; First Aid',\n",
       " 'Schools & Teaching',\n",
       " 'Schools &amp; Teaching',\n",
       " 'Science & Math',\n",
       " 'Science & Mathematics',\n",
       " 'Science &amp; Math',\n",
       " 'Science &amp; Mathematics',\n",
       " 'Science Fiction',\n",
       " 'Science Fiction & Fantasy',\n",
       " 'Science Fiction &amp; Fantasy',\n",
       " 'Science for Kids',\n",
       " 'Science, Nature & How It Works',\n",
       " 'Science, Nature &amp; How It Works',\n",
       " 'Sculpture',\n",
       " 'Security & Encryption',\n",
       " 'Security &amp; Encryption',\n",
       " 'Self-Esteem',\n",
       " 'Self-Help',\n",
       " 'Sex',\n",
       " 'Sexual Health',\n",
       " 'Short Stories & Anthologies',\n",
       " 'Short Stories &amp; Anthologies',\n",
       " 'Skills',\n",
       " 'Small Business & Entrepreneurship',\n",
       " 'Small Business &amp; Entrepreneurship',\n",
       " 'Soccer',\n",
       " 'Social Issues',\n",
       " 'Social Sciences',\n",
       " 'Sociology',\n",
       " 'Software',\n",
       " 'South America',\n",
       " 'Special Diet',\n",
       " 'Special Needs',\n",
       " 'Specialties',\n",
       " 'Specialty Travel',\n",
       " 'Specific Groups',\n",
       " 'Spiritual',\n",
       " 'Sports & Outdoors',\n",
       " 'Sports &amp; Outdoors',\n",
       " 'Sports Health &amp; Safety',\n",
       " 'Stress Management',\n",
       " 'Study & Teaching',\n",
       " 'Study &amp; Teaching',\n",
       " 'Study Guides',\n",
       " 'Studying & Workbooks',\n",
       " 'Studying &amp; Workbooks',\n",
       " 'Success',\n",
       " 'Tax Law',\n",
       " 'Taxation',\n",
       " 'Technology',\n",
       " 'Teen & Young Adult',\n",
       " 'Teen &amp; Young Adult',\n",
       " 'Teen Health',\n",
       " 'Television',\n",
       " 'Test Prep & Study Guides',\n",
       " 'Test Prep &amp; Study Guides',\n",
       " 'Test Preparation',\n",
       " 'Theology',\n",
       " 'Thrillers & Suspense',\n",
       " 'Thrillers &amp; Suspense',\n",
       " 'Training',\n",
       " 'Transportation',\n",
       " 'Travel',\n",
       " 'Travel Writing',\n",
       " 'Travelers & Explorers',\n",
       " 'Travelers &amp; Explorers',\n",
       " 'Trivia & Fun Facts',\n",
       " 'Trivia &amp; Fun Facts',\n",
       " 'True Crime',\n",
       " 'United States',\n",
       " 'Vaccinations',\n",
       " 'Vegetarian & Vegan',\n",
       " 'Vegetarian &amp; Vegan',\n",
       " 'Water Sports',\n",
       " 'Web Development & Design',\n",
       " 'Web Development &amp; Design',\n",
       " 'Weddings',\n",
       " 'Winter Sports',\n",
       " 'Women &amp; Business',\n",
       " \"Women's Fiction\",\n",
       " \"Women's Health\",\n",
       " \"Women's Studies\",\n",
       " 'Words, Language & Grammar',\n",
       " 'Words, Language &amp; Grammar',\n",
       " 'World',\n",
       " 'World Literature',\n",
       " 'Worship & Devotion',\n",
       " 'Worship &amp; Devotion',\n",
       " 'Writing, Research & Publishing Guides',\n",
       " 'Writing, Research &amp; Publishing Guides'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_set = set()\n",
    "for i in range(len(meta)):\n",
    "#     print(meta[i]['category'])\n",
    "    for cat in meta[i]['category']:\n",
    "        genre_set.add(cat)\n",
    "genre_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_no = []\n",
    "title = []\n",
    "for i in range(len(meta)):\n",
    "    asin_no.append(meta[i]['asin'])\n",
    "    title.append(meta[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000092878</th>\n",
       "      <td>Biology Gods Living Creation Third Edition 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000047715X</th>\n",
       "      <td>Mksap 16 Audio Companion: Medical Knowledge Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000004545</th>\n",
       "      <td>Flex! Discography of North American Punk, Hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000013765</th>\n",
       "      <td>Heavenly Highway Hymns: Shaped-Note Hymnal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000000116</th>\n",
       "      <td>Georgina Goodman Nelson Womens Size 8.5 Purple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0888646445</th>\n",
       "      <td>You Haven&amp;rsquo;t Changed a Bit, and Other Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0888644817</th>\n",
       "      <td>Deep Alberta: Fossil Facts and Dinosaur Digs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0888550081</th>\n",
       "      <td>Canadian Military Handguns 1855-1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0888645341</th>\n",
       "      <td>Prodigal Daughter: A Journey to Byzantium (Way...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0888641400</th>\n",
       "      <td>Rose Gardening on the Prairies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000001 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title\n",
       "asin                                                         \n",
       "0000092878  Biology Gods Living Creation Third Edition 10 ...\n",
       "000047715X  Mksap 16 Audio Companion: Medical Knowledge Se...\n",
       "0000004545  Flex! Discography of North American Punk, Hard...\n",
       "0000013765         Heavenly Highway Hymns: Shaped-Note Hymnal\n",
       "0000000116  Georgina Goodman Nelson Womens Size 8.5 Purple...\n",
       "...                                                       ...\n",
       "0888646445  You Haven&rsquo;t Changed a Bit, and Other Sto...\n",
       "0888644817       Deep Alberta: Fossil Facts and Dinosaur Digs\n",
       "0888550081               Canadian Military Handguns 1855-1985\n",
       "0888645341  Prodigal Daughter: A Journey to Byzantium (Way...\n",
       "0888641400                     Rose Gardening on the Prairies\n",
       "\n",
       "[1000001 rows x 1 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(asin_no, title)), \n",
    "               columns =['asin', 'title'])\n",
    "df.set_index('asin',inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in set(df1.index):\n",
    "    try:\n",
    "        df1.loc[df1.index == x, \"title\"] = df.loc[x]['title']\n",
    "    except:\n",
    "        df1.loc[df1.index == x, \"title\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001713353</th>\n",
       "      <td>The King, the Mice and the Cheese by Nancy Gur...</td>\n",
       "      <td>A story children will love and learn from</td>\n",
       "      <td>5.0</td>\n",
       "      <td>King, the Mice and the Cheese (Beginner Books)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001713353</th>\n",
       "      <td>The kids loved it!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5.0</td>\n",
       "      <td>King, the Mice and the Cheese (Beginner Books)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001713353</th>\n",
       "      <td>My students (3 &amp; 4 year olds) loved this book!...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5.0</td>\n",
       "      <td>King, the Mice and the Cheese (Beginner Books)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001713353</th>\n",
       "      <td>LOVE IT</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5.0</td>\n",
       "      <td>King, the Mice and the Cheese (Beginner Books)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001713353</th>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5.0</td>\n",
       "      <td>King, the Mice and the Cheese (Beginner Books)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0061709565</th>\n",
       "      <td>Things I liked:\\n\\n-Nice, easy, short read. Th...</td>\n",
       "      <td>Good If You're Looking For A Shorter Book</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Full Moon (Dark Guardian, Book 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0061709565</th>\n",
       "      <td>Full Moon is the second  book in the Dark Guar...</td>\n",
       "      <td>Worth ten bucks.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Full Moon (Dark Guardian, Book 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>006170007X</th>\n",
       "      <td>Ransom my Heart is the first Meg Cabot book I ...</td>\n",
       "      <td>Sexy Medieval Romance (B+ Grade)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Ransom My Heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>006170007X</th>\n",
       "      <td>When Mellana needs money for her dowry so that...</td>\n",
       "      <td>Medieval romance that will make you smile at t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ransom My Heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>006170007X</th>\n",
       "      <td>Mellana, Fiona's sister has spent her whole do...</td>\n",
       "      <td>Review</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Ransom My Heart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000001 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       review  \\\n",
       "asin                                                            \n",
       "0001713353  The King, the Mice and the Cheese by Nancy Gur...   \n",
       "0001713353                                 The kids loved it!   \n",
       "0001713353  My students (3 & 4 year olds) loved this book!...   \n",
       "0001713353                                            LOVE IT   \n",
       "0001713353                                             Great!   \n",
       "...                                                       ...   \n",
       "0061709565  Things I liked:\\n\\n-Nice, easy, short read. Th...   \n",
       "0061709565  Full Moon is the second  book in the Dark Guar...   \n",
       "006170007X  Ransom my Heart is the first Meg Cabot book I ...   \n",
       "006170007X  When Mellana needs money for her dowry so that...   \n",
       "006170007X  Mellana, Fiona's sister has spent her whole do...   \n",
       "\n",
       "                                                      summary  label  \\\n",
       "asin                                                                   \n",
       "0001713353          A story children will love and learn from    5.0   \n",
       "0001713353                                         Five Stars    5.0   \n",
       "0001713353                                         Five Stars    5.0   \n",
       "0001713353                                         Five Stars    5.0   \n",
       "0001713353                                         Five Stars    5.0   \n",
       "...                                                       ...    ...   \n",
       "0061709565          Good If You're Looking For A Shorter Book    3.0   \n",
       "0061709565                                   Worth ten bucks.    4.0   \n",
       "006170007X                   Sexy Medieval Romance (B+ Grade)    4.0   \n",
       "006170007X  Medieval romance that will make you smile at t...    5.0   \n",
       "006170007X                                             Review    4.0   \n",
       "\n",
       "                                                     title  \n",
       "asin                                                        \n",
       "0001713353  King, the Mice and the Cheese (Beginner Books)  \n",
       "0001713353  King, the Mice and the Cheese (Beginner Books)  \n",
       "0001713353  King, the Mice and the Cheese (Beginner Books)  \n",
       "0001713353  King, the Mice and the Cheese (Beginner Books)  \n",
       "0001713353  King, the Mice and the Cheese (Beginner Books)  \n",
       "...                                                    ...  \n",
       "0061709565               Full Moon (Dark Guardian, Book 2)  \n",
       "0061709565               Full Moon (Dark Guardian, Book 2)  \n",
       "006170007X                                 Ransom My Heart  \n",
       "006170007X                                 Ransom My Heart  \n",
       "006170007X                                 Ransom My Heart  \n",
       "\n",
       "[1000001 rows x 4 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_train = df1[df1['label']<3].copy()[:60000]\n",
    "below_valid = df1[df1['label']<3].copy()[60000:70000]\n",
    "below_test = df1[df1['label']<3].copy()[70000:80982]\n",
    "equal_train = df1[df1['label']==3].copy()[:60000]\n",
    "equal_valid = df1[df1['label']==3].copy()[60000:70000]\n",
    "equal_test = df1[df1['label']==3].copy()[70000:80982]\n",
    "above_train = df1[df1['label']>3].copy()[:60000]\n",
    "above_valid = df1[df1['label']>3].copy()[60000:70000]\n",
    "above_test = df1[df1['label']>3].copy()[70000:80982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([below_train, equal_train, above_train])\n",
    "valid = pd.concat([below_valid, equal_valid, above_valid])\n",
    "test = pd.concat([below_test, equal_test, above_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180000, 4), (30000, 4), (32946, 4))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"books/train.csv\")\n",
    "valid.to_csv(\"books/valid.csv\")\n",
    "test.to_csv(\"books/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 2., 4., 1.])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train[\"label\"]<3,'label']=0\n",
    "train.loc[train[\"label\"]>=3,'label']=1\n",
    "# train.loc[train[\"label\"]==3,'label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.loc[valid[\"label\"]<3,'label']=0\n",
    "valid.loc[valid[\"label\"]>=3,'label']=1\n",
    "# valid.loc[valid[\"label\"]==3,'label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test[\"label\"]<3,'label']=0\n",
    "test.loc[test[\"label\"]>=3,'label']=1\n",
    "# test.loc[test[\"label\"]==3,'label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"books/train.csv\")\n",
    "valid = pd.read_csv(\"books/valid.csv\")\n",
    "test = pd.read_csv(\"books/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def text_preproc(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = \" \".join(x.split(\"<br />\"))\n",
    "    x = \" \".join(x.split(\"\\n\"))\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train.review.astype(str).apply(text_preproc)\n",
    "valid['clean_text'] = valid.review.astype(str).apply(text_preproc)\n",
    "test['clean_text'] = test.review.astype(str).apply(text_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "30000\n",
      "32946\n"
     ]
    }
   ],
   "source": [
    "voc_size = 10000\n",
    "onehot_repr = []\n",
    "onehot_repr=onehot_repr+[one_hot(sents,voc_size)for sents in train[\"clean_text\"]] \n",
    "print(len(onehot_repr))\n",
    "onehot_reprv = []\n",
    "onehot_reprv=onehot_reprv+[one_hot(sents,voc_size)for sents in valid[\"clean_text\"]] \n",
    "print(len(onehot_reprv))\n",
    "onehot_reprt = []\n",
    "onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in test[\"clean_text\"]] \n",
    "print(len(onehot_reprt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 1942 3692  457]\n",
      " [   0    0    0 ... 2940  457 5168]\n",
      " [   0    0    0 ... 1210 3692 2760]\n",
      " ...\n",
      " [   0    0    0 ... 8387 3918 3692]\n",
      " [   0    0    0 ...  573 1372 4548]\n",
      " [   0    0    0 ... 9312 4380 8760]]\n",
      "(180000, 50) (180000,)\n",
      "[[6014 5677 3080 ... 2583 9653 4356]\n",
      " [7198  179 8490 ... 5277 4986 7315]\n",
      " [2400 4952 7810 ... 3734 3807 5265]\n",
      " ...\n",
      " [2599 3429 2533 ... 3692  115 1002]\n",
      " [   0    0    0 ... 6009 8116 6649]\n",
      " [   0    0    0 ...    0 4846 3692]]\n",
      "(30000, 50) (30000,)\n",
      "[[   0    0    0 ... 3109 5382 1661]\n",
      " [9925  457 1428 ... 4929 8802 6545]\n",
      " [   0    0    0 ...  457 7031  318]\n",
      " ...\n",
      " [   0    0    0 ... 5797 1530 5382]\n",
      " [   0    0    0 ... 3091    5 8268]\n",
      " [   0    0    0 ...    0  166 3692]]\n",
      "(32946, 50) (32946,)\n"
     ]
    }
   ],
   "source": [
    "sent_length=50\n",
    "train_x=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "train_y = train.label\n",
    "print(train_x)\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "val_x=pad_sequences(onehot_reprv,padding='pre',maxlen=sent_length)\n",
    "val_y = valid.label\n",
    "print(val_x)\n",
    "print(val_x.shape, val_y.shape)\n",
    "\n",
    "test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)\n",
    "test_y = test.label\n",
    "print(test_x)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,366,593\n",
      "Trainable params: 1,366,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,100,input_length=sent_length))\n",
    "model.compile('adam','mse')\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 30000 samples\n",
      "Epoch 1/10\n",
      "180000/180000 [==============================] - 142s 789us/sample - loss: 0.4712 - accuracy: 0.7722 - val_loss: 0.4644 - val_accuracy: 0.7729\n",
      "Epoch 2/10\n",
      "180000/180000 [==============================] - 137s 763us/sample - loss: 0.4180 - accuracy: 0.8041 - val_loss: 0.4684 - val_accuracy: 0.7712\n",
      "Epoch 3/10\n",
      "180000/180000 [==============================] - 134s 746us/sample - loss: 0.3899 - accuracy: 0.8214 - val_loss: 0.4889 - val_accuracy: 0.7677\n",
      "Epoch 4/10\n",
      "180000/180000 [==============================] - 146s 814us/sample - loss: 0.3620 - accuracy: 0.8362 - val_loss: 0.5237 - val_accuracy: 0.7580\n",
      "Epoch 5/10\n",
      "180000/180000 [==============================] - 139s 770us/sample - loss: 0.3332 - accuracy: 0.8516 - val_loss: 0.5507 - val_accuracy: 0.7548\n",
      "Epoch 6/10\n",
      "180000/180000 [==============================] - 142s 789us/sample - loss: 0.3054 - accuracy: 0.8662 - val_loss: 0.5946 - val_accuracy: 0.7464\n",
      "Epoch 7/10\n",
      "180000/180000 [==============================] - 139s 771us/sample - loss: 0.2807 - accuracy: 0.8783 - val_loss: 0.6437 - val_accuracy: 0.7461\n",
      "Epoch 8/10\n",
      "180000/180000 [==============================] - 150s 836us/sample - loss: 0.2587 - accuracy: 0.8882 - val_loss: 0.7263 - val_accuracy: 0.7447\n",
      "Epoch 9/10\n",
      "180000/180000 [==============================] - 144s 799us/sample - loss: 0.2385 - accuracy: 0.8971 - val_loss: 0.7971 - val_accuracy: 0.7421\n",
      "Epoch 10/10\n",
      "180000/180000 [==============================] - 117s 649us/sample - loss: 0.2200 - accuracy: 0.9056 - val_loss: 0.9118 - val_accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x241ffd4ccf8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=256, \n",
    "          epochs=10,validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32946/32946 [==============================] - 25s 765us/sample - loss: 0.8309 - accuracy: 0.7558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8309454937369773, 0.7557822]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.models.save_model(model,\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_reprx = []\n",
    "s = \"What a classy book\"\n",
    "s = [s]\n",
    "onehot_reprx=onehot_reprx+[one_hot(sents,voc_size)for sents in s] \n",
    "testing_x=pad_sequences(onehot_reprx,padding='pre',maxlen=sent_length)\n",
    "testing_x.shape\n",
    "model.predict_classes(testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
