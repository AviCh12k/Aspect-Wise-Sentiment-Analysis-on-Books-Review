{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone needed to make a car payment... this i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guidelines state that a comment must conta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Man, I loved this movie! This really takes me ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Recovery is an incredibly moving piece of work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>You can take the crook out of the joint, but i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>FUTZ is the only show preserved from the exper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>\"The Mother\" tells of a recently widowed mid-6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     It's been about 14 years since Sharon Stone aw...      0\n",
       "1     someone needed to make a car payment... this i...      0\n",
       "2     The Guidelines state that a comment must conta...      0\n",
       "3     This movie is a muddled mish-mash of clichés f...      0\n",
       "4     Before Stan Laurel became the smaller half of ...      0\n",
       "...                                                 ...    ...\n",
       "4995  Man, I loved this movie! This really takes me ...      1\n",
       "4996  Recovery is an incredibly moving piece of work...      1\n",
       "4997  You can take the crook out of the joint, but i...      1\n",
       "4998  FUTZ is the only show preserved from the exper...      1\n",
       "4999  \"The Mother\" tells of a recently widowed mid-6...      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB/valid.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "# wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproc(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df.text.apply(text_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>years since sharon stone awarded viewers leg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone needed to make a car payment... this i...</td>\n",
       "      <td>0</td>\n",
       "      <td>someone needed make car payment truly awful ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guidelines state that a comment must conta...</td>\n",
       "      <td>0</td>\n",
       "      <td>guidelines state comment must contain minimum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
       "      <td>0</td>\n",
       "      <td>movie muddled mish mash clichs recent cinema p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>stan laurel became smaller half all time great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Man, I loved this movie! This really takes me ...</td>\n",
       "      <td>1</td>\n",
       "      <td>man loved movie really takes back kid days tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Recovery is an incredibly moving piece of work...</td>\n",
       "      <td>1</td>\n",
       "      <td>recovery incredibly moving piece work handling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>You can take the crook out of the joint, but i...</td>\n",
       "      <td>1</td>\n",
       "      <td>take crook joint seems exceedingly difficult t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>FUTZ is the only show preserved from the exper...</td>\n",
       "      <td>1</td>\n",
       "      <td>futz show preserved experimental theatre movem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>\"The Mother\" tells of a recently widowed mid-6...</td>\n",
       "      <td>1</td>\n",
       "      <td>the mother tells recently widowed mid mother ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "0     It's been about 14 years since Sharon Stone aw...      0   \n",
       "1     someone needed to make a car payment... this i...      0   \n",
       "2     The Guidelines state that a comment must conta...      0   \n",
       "3     This movie is a muddled mish-mash of clichés f...      0   \n",
       "4     Before Stan Laurel became the smaller half of ...      0   \n",
       "...                                                 ...    ...   \n",
       "4995  Man, I loved this movie! This really takes me ...      1   \n",
       "4996  Recovery is an incredibly moving piece of work...      1   \n",
       "4997  You can take the crook out of the joint, but i...      1   \n",
       "4998  FUTZ is the only show preserved from the exper...      1   \n",
       "4999  \"The Mother\" tells of a recently widowed mid-6...      1   \n",
       "\n",
       "                                             clean_text  \n",
       "0      years since sharon stone awarded viewers leg ...  \n",
       "1     someone needed make car payment truly awful ma...  \n",
       "2     guidelines state comment must contain minimum ...  \n",
       "3     movie muddled mish mash clichs recent cinema p...  \n",
       "4     stan laurel became smaller half all time great...  \n",
       "...                                                 ...  \n",
       "4995  man loved movie really takes back kid days tea...  \n",
       "4996  recovery incredibly moving piece work handling...  \n",
       "4997  take crook joint seems exceedingly difficult t...  \n",
       "4998  futz show preserved experimental theatre movem...  \n",
       "4999   the mother tells recently widowed mid mother ...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"text\"], inplace= True)\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e132bb067738>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IMDB/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"IMDB/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"IMDB/train.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2ba30f8b5d14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-2ba30f8b5d14>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "x = [len(word_tokenize(sent)) for sent in df1[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.3992"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_repr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "onehot_repr = []\n",
    "onehot_repr=onehot_repr+[one_hot(sents,voc_size)for sents in df[\"text\"][:40000]] \n",
    "print(len(onehot_repr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 535 9678 9515 ... 4063 8168 8293]\n",
      " [7581 8686 2861 ... 3716 1002 9442]\n",
      " [6538 7536  700 ... 7389 6531 1745]\n",
      " ...\n",
      " [ 785 9573 4604 ... 8232 4311 8592]\n",
      " [7536 9678 6696 ... 8686  362 3215]\n",
      " [5277 9678 4750 ... 4399 2279 6231]]\n"
     ]
    }
   ],
   "source": [
    "sent_length=100\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,100,input_length=sent_length))\n",
    "model.compile('adam','mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length=100\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,100,input_length=sent_length))\n",
    "model.compile('adam','mse')\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,366,593\n",
      "Trainable params: 1,366,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 100), (40000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = embedded_docs\n",
    "train_y = df.label\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone needed to make a car payment... this i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guidelines state that a comment must conta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Man, I loved this movie! This really takes me ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Recovery is an incredibly moving piece of work...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>You can take the crook out of the joint, but i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>FUTZ is the only show preserved from the exper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>\"The Mother\" tells of a recently widowed mid-6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     It's been about 14 years since Sharon Stone aw...      0\n",
       "1     someone needed to make a car payment... this i...      0\n",
       "2     The Guidelines state that a comment must conta...      0\n",
       "3     This movie is a muddled mish-mash of clichés f...      0\n",
       "4     Before Stan Laurel became the smaller half of ...      0\n",
       "...                                                 ...    ...\n",
       "4995  Man, I loved this movie! This really takes me ...      1\n",
       "4996  Recovery is an incredibly moving piece of work...      1\n",
       "4997  You can take the crook out of the joint, but i...      1\n",
       "4998  FUTZ is the only show preserved from the exper...      1\n",
       "4999  \"The Mother\" tells of a recently widowed mid-6...      1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = pd.read_csv(\"IMDB/valid.csv\")\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_reprv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "onehot_reprv=onehot_reprv+[one_hot(sents,voc_size)for sents in df[\"text\"][:5000]] \n",
    "print(len(onehot_reprv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 100) (5000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_x=pad_sequences(onehot_reprv,padding='pre',maxlen=sent_length)\n",
    "val_y = val.label\n",
    "print(val_x.shape,val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 696s 17ms/sample - loss: 0.4352 - accuracy: 0.7964 - val_loss: 1.3390 - val_accuracy: 0.5012\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 672s 17ms/sample - loss: 0.3036 - accuracy: 0.8722 - val_loss: 1.4820 - val_accuracy: 0.4974\n",
      "Epoch 3/3\n",
      " 2112/40000 [>.............................] - ETA: 11:07 - loss: 0.1959 - accuracy: 0.9308"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-83832edbd876>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(train_x, train_y, batch_size=32, \n\u001b[1;32m----> 2\u001b[1;33m           epochs=3,validation_data=(val_x,val_y))\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=32, \n",
    "          epochs=3,validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most interesting thing about Miryang (Secr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>This is the kind of picture John Lassiter woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>A MUST SEE! I saw WHIPPED at a press screening...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>NBC should be ashamed. I wouldn't allow my chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>This movie is a clumsy mishmash of various gho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Formula movie about the illegitimate son of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     I always wrote this series off as being a comp...      0\n",
       "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2     This movie was so poorly written and directed ...      0\n",
       "3     The most interesting thing about Miryang (Secr...      1\n",
       "4     when i first read about \"berlin am meer\" i did...      0\n",
       "...                                                 ...    ...\n",
       "4995  This is the kind of picture John Lassiter woul...      1\n",
       "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
       "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
       "4998  This movie is a clumsy mishmash of various gho...      0\n",
       "4999  Formula movie about the illegitimate son of a ...      0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"IMDB/test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_reprt = []\n",
    "s = []\n",
    "for x in test[\"text\"]:\n",
    "    s.append(\"\".join(x.split(\"<br />\")))\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in s] \n",
    "print(len(onehot_reprt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 100) (5000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)\n",
    "test_y = test.label\n",
    "print(test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 9s 2ms/sample - loss: 0.4464 - accuracy: 0.8528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4463780487537384, 0.8528]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict_classes(test_x)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2][0]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_reprt = []\n",
    "sentence = [s]\n",
    "onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in s] \n",
    "test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict_classes(test_x)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "4995    1\n",
       "4996    1\n",
       "4997    0\n",
       "4998    0\n",
       "4999    0\n",
       "Name: label, Length: 5000, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n",
      "18\n",
      "28\n",
      "35\n",
      "40\n",
      "61\n",
      "70\n",
      "81\n",
      "93\n",
      "98\n",
      "99\n",
      "100\n",
      "102\n",
      "104\n",
      "111\n",
      "114\n",
      "128\n",
      "132\n",
      "134\n",
      "138\n",
      "139\n",
      "145\n",
      "147\n",
      "160\n",
      "165\n",
      "169\n",
      "173\n",
      "174\n",
      "180\n",
      "183\n",
      "185\n",
      "205\n",
      "207\n",
      "209\n",
      "212\n",
      "213\n",
      "217\n",
      "221\n",
      "224\n",
      "232\n",
      "235\n",
      "241\n",
      "246\n",
      "250\n",
      "251\n",
      "256\n",
      "261\n",
      "263\n",
      "266\n",
      "268\n",
      "272\n",
      "277\n",
      "278\n",
      "296\n",
      "309\n",
      "313\n",
      "321\n",
      "325\n",
      "333\n",
      "340\n",
      "348\n",
      "351\n",
      "359\n",
      "361\n",
      "363\n",
      "370\n",
      "373\n",
      "377\n",
      "379\n",
      "381\n",
      "383\n",
      "386\n",
      "419\n",
      "422\n",
      "432\n",
      "438\n",
      "448\n",
      "451\n",
      "460\n",
      "477\n",
      "480\n",
      "485\n",
      "490\n",
      "494\n",
      "499\n",
      "506\n",
      "509\n",
      "513\n",
      "515\n",
      "517\n",
      "527\n",
      "532\n",
      "534\n",
      "540\n",
      "542\n",
      "553\n",
      "571\n",
      "580\n",
      "587\n",
      "589\n",
      "592\n",
      "594\n",
      "595\n",
      "601\n",
      "609\n",
      "615\n",
      "631\n",
      "636\n",
      "641\n",
      "642\n",
      "651\n",
      "661\n",
      "663\n",
      "665\n",
      "675\n",
      "685\n",
      "688\n",
      "691\n",
      "693\n",
      "707\n",
      "709\n",
      "710\n",
      "716\n",
      "729\n",
      "736\n",
      "740\n",
      "750\n",
      "756\n",
      "759\n",
      "762\n",
      "785\n",
      "792\n",
      "810\n",
      "811\n",
      "817\n",
      "844\n",
      "848\n",
      "853\n",
      "857\n",
      "867\n",
      "871\n",
      "895\n",
      "901\n",
      "902\n",
      "913\n",
      "922\n",
      "939\n",
      "954\n",
      "960\n",
      "961\n",
      "963\n",
      "970\n",
      "994\n",
      "997\n",
      "1000\n",
      "1006\n",
      "1008\n",
      "1017\n",
      "1019\n",
      "1021\n",
      "1042\n",
      "1043\n",
      "1046\n",
      "1049\n",
      "1050\n",
      "1056\n",
      "1058\n",
      "1076\n",
      "1077\n",
      "1080\n",
      "1103\n",
      "1117\n",
      "1120\n",
      "1126\n",
      "1127\n",
      "1132\n",
      "1138\n",
      "1143\n",
      "1148\n",
      "1162\n",
      "1163\n",
      "1170\n",
      "1178\n",
      "1179\n",
      "1182\n",
      "1189\n",
      "1195\n",
      "1202\n",
      "1205\n",
      "1208\n",
      "1209\n",
      "1217\n",
      "1240\n",
      "1243\n",
      "1245\n",
      "1251\n",
      "1256\n",
      "1271\n",
      "1284\n",
      "1288\n",
      "1297\n",
      "1305\n",
      "1307\n",
      "1309\n",
      "1318\n",
      "1342\n",
      "1354\n",
      "1370\n",
      "1388\n",
      "1399\n",
      "1413\n",
      "1431\n",
      "1433\n",
      "1438\n",
      "1441\n",
      "1442\n",
      "1456\n",
      "1462\n",
      "1473\n",
      "1477\n",
      "1478\n",
      "1487\n",
      "1488\n",
      "1499\n",
      "1510\n",
      "1511\n",
      "1515\n",
      "1518\n",
      "1530\n",
      "1532\n",
      "1538\n",
      "1545\n",
      "1546\n",
      "1551\n",
      "1555\n",
      "1556\n",
      "1565\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1573\n",
      "1604\n",
      "1605\n",
      "1609\n",
      "1616\n",
      "1619\n",
      "1627\n",
      "1628\n",
      "1633\n",
      "1636\n",
      "1637\n",
      "1649\n",
      "1657\n",
      "1665\n",
      "1667\n",
      "1676\n",
      "1678\n",
      "1680\n",
      "1685\n",
      "1688\n",
      "1706\n",
      "1718\n",
      "1721\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1741\n",
      "1750\n",
      "1758\n",
      "1761\n",
      "1768\n",
      "1769\n",
      "1772\n",
      "1779\n",
      "1785\n",
      "1789\n",
      "1800\n",
      "1811\n",
      "1829\n",
      "1837\n",
      "1852\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1866\n",
      "1870\n",
      "1877\n",
      "1880\n",
      "1890\n",
      "1903\n",
      "1909\n",
      "1911\n",
      "1915\n",
      "1922\n",
      "1930\n",
      "1935\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1957\n",
      "1981\n",
      "1991\n",
      "1994\n",
      "1999\n",
      "2006\n",
      "2017\n",
      "2021\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2050\n",
      "2053\n",
      "2055\n",
      "2059\n",
      "2072\n",
      "2083\n",
      "2092\n",
      "2115\n",
      "2128\n",
      "2132\n",
      "2142\n",
      "2144\n",
      "2149\n",
      "2151\n",
      "2154\n",
      "2156\n",
      "2166\n",
      "2169\n",
      "2170\n",
      "2172\n",
      "2173\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2182\n",
      "2186\n",
      "2193\n",
      "2203\n",
      "2212\n",
      "2225\n",
      "2227\n",
      "2278\n",
      "2287\n",
      "2290\n",
      "2291\n",
      "2294\n",
      "2301\n",
      "2302\n",
      "2312\n",
      "2319\n",
      "2321\n",
      "2327\n",
      "2328\n",
      "2353\n",
      "2361\n",
      "2364\n",
      "2368\n",
      "2371\n",
      "2374\n",
      "2380\n",
      "2381\n",
      "2386\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2397\n",
      "2407\n",
      "2415\n",
      "2417\n",
      "2424\n",
      "2440\n",
      "2452\n",
      "2457\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2489\n",
      "2498\n",
      "2512\n",
      "2514\n",
      "2516\n",
      "2518\n",
      "2523\n",
      "2528\n",
      "2542\n",
      "2558\n",
      "2565\n",
      "2566\n",
      "2576\n",
      "2579\n",
      "2584\n",
      "2603\n",
      "2605\n",
      "2610\n",
      "2615\n",
      "2625\n",
      "2639\n",
      "2650\n",
      "2666\n",
      "2680\n",
      "2695\n",
      "2698\n",
      "2705\n",
      "2709\n",
      "2713\n",
      "2720\n",
      "2736\n",
      "2745\n",
      "2753\n",
      "2754\n",
      "2762\n",
      "2768\n",
      "2769\n",
      "2776\n",
      "2781\n",
      "2788\n",
      "2801\n",
      "2803\n",
      "2811\n",
      "2813\n",
      "2825\n",
      "2836\n",
      "2842\n",
      "2876\n",
      "2887\n",
      "2923\n",
      "2925\n",
      "2929\n",
      "2932\n",
      "2938\n",
      "2948\n",
      "2951\n",
      "2959\n",
      "2962\n",
      "2965\n",
      "2970\n",
      "2974\n",
      "2984\n",
      "2993\n",
      "2995\n",
      "2999\n",
      "3001\n",
      "3002\n",
      "3006\n",
      "3010\n",
      "3019\n",
      "3032\n",
      "3055\n",
      "3066\n",
      "3073\n",
      "3075\n",
      "3077\n",
      "3081\n",
      "3084\n",
      "3095\n",
      "3100\n",
      "3117\n",
      "3121\n",
      "3150\n",
      "3153\n",
      "3161\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3176\n",
      "3179\n",
      "3197\n",
      "3200\n",
      "3212\n",
      "3221\n",
      "3245\n",
      "3250\n",
      "3255\n",
      "3258\n",
      "3263\n",
      "3291\n",
      "3293\n",
      "3297\n",
      "3299\n",
      "3312\n",
      "3320\n",
      "3325\n",
      "3327\n",
      "3337\n",
      "3359\n",
      "3362\n",
      "3369\n",
      "3371\n",
      "3380\n",
      "3398\n",
      "3399\n",
      "3404\n",
      "3412\n",
      "3416\n",
      "3422\n",
      "3423\n",
      "3437\n",
      "3448\n",
      "3455\n",
      "3462\n",
      "3465\n",
      "3466\n",
      "3475\n",
      "3485\n",
      "3497\n",
      "3520\n",
      "3522\n",
      "3527\n",
      "3536\n",
      "3541\n",
      "3542\n",
      "3546\n",
      "3548\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3568\n",
      "3577\n",
      "3579\n",
      "3582\n",
      "3587\n",
      "3596\n",
      "3597\n",
      "3602\n",
      "3605\n",
      "3610\n",
      "3618\n",
      "3619\n",
      "3628\n",
      "3631\n",
      "3633\n",
      "3634\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3662\n",
      "3672\n",
      "3695\n",
      "3697\n",
      "3707\n",
      "3725\n",
      "3738\n",
      "3739\n",
      "3751\n",
      "3757\n",
      "3758\n",
      "3762\n",
      "3779\n",
      "3791\n",
      "3804\n",
      "3810\n",
      "3817\n",
      "3821\n",
      "3835\n",
      "3838\n",
      "3858\n",
      "3859\n",
      "3861\n",
      "3875\n",
      "3888\n",
      "3893\n",
      "3908\n",
      "3910\n",
      "3911\n",
      "3914\n",
      "3925\n",
      "3945\n",
      "3955\n",
      "3968\n",
      "3972\n",
      "3973\n",
      "3989\n",
      "3990\n",
      "3992\n",
      "4003\n",
      "4008\n",
      "4010\n",
      "4016\n",
      "4018\n",
      "4032\n",
      "4040\n",
      "4041\n",
      "4046\n",
      "4055\n",
      "4059\n",
      "4061\n",
      "4064\n",
      "4066\n",
      "4069\n",
      "4077\n",
      "4078\n",
      "4084\n",
      "4086\n",
      "4096\n",
      "4106\n",
      "4107\n",
      "4114\n",
      "4117\n",
      "4119\n",
      "4124\n",
      "4127\n",
      "4142\n",
      "4147\n",
      "4148\n",
      "4151\n",
      "4154\n",
      "4169\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4203\n",
      "4210\n",
      "4213\n",
      "4214\n",
      "4223\n",
      "4229\n",
      "4231\n",
      "4242\n",
      "4258\n",
      "4260\n",
      "4265\n",
      "4276\n",
      "4280\n",
      "4292\n",
      "4293\n",
      "4303\n",
      "4305\n",
      "4306\n",
      "4308\n",
      "4316\n",
      "4326\n",
      "4336\n",
      "4345\n",
      "4347\n",
      "4349\n",
      "4350\n",
      "4367\n",
      "4368\n",
      "4373\n",
      "4381\n",
      "4394\n",
      "4400\n",
      "4403\n",
      "4405\n",
      "4415\n",
      "4421\n",
      "4432\n",
      "4438\n",
      "4439\n",
      "4441\n",
      "4449\n",
      "4455\n",
      "4457\n",
      "4458\n",
      "4463\n",
      "4472\n",
      "4474\n",
      "4486\n",
      "4504\n",
      "4508\n",
      "4509\n",
      "4511\n",
      "4513\n",
      "4523\n",
      "4526\n",
      "4537\n",
      "4554\n",
      "4565\n",
      "4578\n",
      "4588\n",
      "4589\n",
      "4592\n",
      "4601\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4617\n",
      "4621\n",
      "4626\n",
      "4632\n",
      "4653\n",
      "4657\n",
      "4664\n",
      "4667\n",
      "4668\n",
      "4681\n",
      "4685\n",
      "4708\n",
      "4711\n",
      "4714\n",
      "4721\n",
      "4726\n",
      "4731\n",
      "4747\n",
      "4764\n",
      "4765\n",
      "4775\n",
      "4788\n",
      "4798\n",
      "4830\n",
      "4836\n",
      "4839\n",
      "4840\n",
      "4843\n",
      "4845\n",
      "4847\n",
      "4851\n",
      "4874\n",
      "4876\n",
      "4879\n",
      "4882\n",
      "4883\n",
      "4885\n",
      "4888\n",
      "4909\n",
      "4912\n",
      "4927\n",
      "4931\n",
      "4944\n",
      "4950\n",
      "4952\n",
      "4957\n",
      "4959\n",
      "4962\n",
      "4975\n",
      "4977\n",
      "4983\n",
      "4985\n",
      "4986\n",
      "4992\n",
      "4997\n",
      "5001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for x in range(5000):\n",
    "    if a[x][0] != test_y[x]:\n",
    "        count+=1\n",
    "        print(x+2)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I always wrote this series off as being a complete stink-fest because Jim Belushi was involved in it, and heavily. But then one day a tragic happenstance occurred. After a White Sox game ended I realized that the remote was all the way on the other side of the room somehow. Now I could have just gotten up and walked across the room to get the remote, or even to the TV to turn the channel. But then why not just get up and walk across the country to watch TV in another state? Nuts to that, I said. So I decided to just hang tight on the couch and take whatever Fate had in store for me. What Fate had in store was an episode of this show, an episode about which I remember very little except that I had once again made a very broad, general sweeping blanket judgment based on zero objective or experiential evidence with nothing whatsoever to back my opinions up with, and once again I was completely right! This show is a total crud-pie! Belushi has all the comedic delivery of a hairy lighthouse foghorn. The women are physically attractive but too Stepford-is to elicit any real feeling from the viewer. There is absolutely no reason to stop yourself from running down to the local TV station with a can of gasoline and a flamethrower and sending every copy of this mutt howling back to hell. <br /><br />Except.. <br /><br />Except for the wonderful comic sty lings of Larry Joe Campbell, America's Greatest Comic Character Actor. This guy plays Belushi's brother-in-law, Andy, and he is gold. How good is he really? Well, aside from being funny, his job is to make Belushi look good. That's like trying to make butt warts look good. But Campbell pulls it off with style. Someone should invent a Nobel Prize in Comic Buffoonery so he can win it every year. Without Larry Joe this show would consist of a slightly vacant looking Courtney Thorne-Smith smacking Belushi over the head with a frying pan while he alternately beats his chest and plays with the straw on the floor of his cage. 5 stars for Larry Joe Campbell designated Comedic Bacon because he improves the flavor of everything he's in!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\".join(s.split(\"<br />\"))\n",
    "s = \"I always wrote this series off as being a complete stink-fest because Jim Belushi was involved in it, and heavily. But then one day a tragic happenstance occurred. After a White Sox game ended I realized that the remote was all the way on the other side of the room somehow. Now I could have just gotten up and walked across the room to get the remote, or even to the TV to turn the channel. But then why not just get up and walk across the country to watch TV in another state? Nuts to that, I said. So I decided to just hang tight on the couch and take whatever Fate had in store for me. What Fate had in store was an episode of this show, an episode about which I remember very little except that I had once again made a very broad, general sweeping blanket judgment based on zero objective or experiential evidence with nothing whatsoever to back my opinions up with, and once again I was completely right! This show is a total crud-pie! Belushi has all the comedic delivery of a hairy lighthouse foghorn. The women are physically attractive but too Stepford-is to elicit any real feeling from the viewer. There is absolutely no reason to stop yourself from running down to the local TV station with a can of gasoline and a flamethrower and sending every copy of this mutt howling back to hell. <br /><br />Except.. <br /><br />Except for the wonderful comic sty lings of Larry Joe Campbell, America's Greatest Comic Character Actor. This guy plays Belushi's brother-in-law, Andy, and he is gold. How good is he really? Well, aside from being funny, his job is to make Belushi look good. That's like trying to make butt warts look good. But Campbell pulls it off with style. Someone should invent a Nobel Prize in Comic Buffoonery so he can win it every year. Without Larry Joe this show would consist of a slightly vacant looking Courtney Thorne-Smith smacking Belushi over the head with a frying pan while he alternately beats his chest and plays with the straw on the floor of his cage. 5 stars for Larry Joe Campbell designated Comedic Bacon because he improves the flavor of everything he's in!\"\n",
    "s = [s]\n",
    "# s = s.split(\".\")\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_reprt = []\n",
    "s = \"I always wrote this series off as being a complete stink-fest because Jim Belushi was involved in it, and heavily. But then one day a tragic happenstance occurred. After a White Sox game ended I realized that the remote was all the way on the other side of the room somehow. Now I could have just gotten up and walked across the room to get the remote, or even to the TV to turn the channel. But then why not just get up and walk across the country to watch TV in another state? Nuts to that, I said. So I decided to just hang tight on the couch and take whatever Fate had in store for me. What Fate had in store was an episode of this show, an episode about which I remember very little except that I had once again made a very broad, general sweeping blanket judgment based on zero objective or experiential evidence with nothing whatsoever to back my opinions up with, and once again I was completely right! This show is a total crud-pie! Belushi has all the comedic delivery of a hairy lighthouse foghorn. The women are physically attractive but too Stepford-is to elicit any real feeling from the viewer. There is absolutely no reason to stop yourself from running down to the local TV station with a can of gasoline and a flamethrower and sending every copy of this mutt howling back to hell. <br /><br />Except.. <br /><br />Except for the wonderful comic sty lings of Larry Joe Campbell, America's Greatest Comic Character Actor. This guy plays Belushi's brother-in-law, Andy, and he is gold. How good is he really? Well, aside from being funny, his job is to make Belushi look good. That's like trying to make butt warts look good. But Campbell pulls it off with style. Someone should invent a Nobel Prize in Comic Buffoonery so he can win it every year. Without Larry Joe this show would consist of a slightly vacant looking Courtney Thorne-Smith smacking Belushi over the head with a frying pan while he alternately beats his chest and plays with the straw on the floor of his cage. 5 stars for Larry Joe Campbell designated Comedic Bacon because he improves the flavor of everything he's in!\"\n",
    "s = [s]\n",
    "onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in s] \n",
    "test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)\n",
    "a = model.predict_classes(test_x)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(a == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_out = []\n",
    "for s in test[\"text\"]:\n",
    "    s = \"\".join(s.split(\"<br />\"))\n",
    "    s = s.split(\".\")\n",
    "    onehot_reprt = []\n",
    "    onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in s] \n",
    "    test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)\n",
    "    a = model.predict_classes(test_x)\n",
    "    l_out.append(1 if np.count_nonzero(a == 1) > len(a)/2 else 0)\n",
    "len(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "6\n",
      "8\n",
      "12\n",
      "14\n",
      "16\n",
      "17\n",
      "19\n",
      "26\n",
      "30\n",
      "36\n",
      "38\n",
      "40\n",
      "48\n",
      "54\n",
      "56\n",
      "57\n",
      "63\n",
      "64\n",
      "69\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "77\n",
      "79\n",
      "82\n",
      "85\n",
      "86\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "94\n",
      "95\n",
      "97\n",
      "100\n",
      "102\n",
      "105\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "123\n",
      "126\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "139\n",
      "140\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "150\n",
      "151\n",
      "154\n",
      "159\n",
      "160\n",
      "161\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "169\n",
      "170\n",
      "175\n",
      "179\n",
      "181\n",
      "183\n",
      "184\n",
      "186\n",
      "187\n",
      "191\n",
      "193\n",
      "194\n",
      "195\n",
      "197\n",
      "200\n",
      "202\n",
      "205\n",
      "208\n",
      "209\n",
      "216\n",
      "221\n",
      "222\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "232\n",
      "237\n",
      "241\n",
      "242\n",
      "249\n",
      "254\n",
      "260\n",
      "262\n",
      "263\n",
      "266\n",
      "268\n",
      "269\n",
      "274\n",
      "278\n",
      "279\n",
      "281\n",
      "283\n",
      "284\n",
      "285\n",
      "289\n",
      "290\n",
      "291\n",
      "296\n",
      "297\n",
      "298\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "306\n",
      "308\n",
      "309\n",
      "316\n",
      "320\n",
      "321\n",
      "322\n",
      "324\n",
      "326\n",
      "328\n",
      "332\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "342\n",
      "344\n",
      "349\n",
      "351\n",
      "352\n",
      "354\n",
      "355\n",
      "358\n",
      "359\n",
      "361\n",
      "363\n",
      "364\n",
      "365\n",
      "367\n",
      "370\n",
      "374\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "388\n",
      "389\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "402\n",
      "406\n",
      "407\n",
      "410\n",
      "411\n",
      "413\n",
      "416\n",
      "419\n",
      "420\n",
      "421\n",
      "426\n",
      "429\n",
      "432\n",
      "433\n",
      "434\n",
      "437\n",
      "440\n",
      "442\n",
      "444\n",
      "446\n",
      "448\n",
      "450\n",
      "451\n",
      "453\n",
      "454\n",
      "460\n",
      "464\n",
      "465\n",
      "466\n",
      "468\n",
      "469\n",
      "471\n",
      "472\n",
      "473\n",
      "475\n",
      "476\n",
      "477\n",
      "479\n",
      "483\n",
      "486\n",
      "490\n",
      "494\n",
      "500\n",
      "502\n",
      "508\n",
      "509\n",
      "510\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "521\n",
      "522\n",
      "525\n",
      "528\n",
      "530\n",
      "531\n",
      "534\n",
      "540\n",
      "545\n",
      "547\n",
      "548\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "562\n",
      "571\n",
      "573\n",
      "574\n",
      "579\n",
      "580\n",
      "581\n",
      "583\n",
      "585\n",
      "590\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "597\n",
      "599\n",
      "601\n",
      "602\n",
      "607\n",
      "608\n",
      "612\n",
      "614\n",
      "615\n",
      "630\n",
      "631\n",
      "633\n",
      "634\n",
      "636\n",
      "637\n",
      "638\n",
      "640\n",
      "642\n",
      "644\n",
      "648\n",
      "655\n",
      "658\n",
      "661\n",
      "662\n",
      "663\n",
      "667\n",
      "671\n",
      "673\n",
      "674\n",
      "675\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "696\n",
      "697\n",
      "700\n",
      "702\n",
      "703\n",
      "705\n",
      "706\n",
      "710\n",
      "711\n",
      "712\n",
      "714\n",
      "715\n",
      "718\n",
      "720\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "732\n",
      "733\n",
      "737\n",
      "740\n",
      "742\n",
      "744\n",
      "746\n",
      "747\n",
      "748\n",
      "751\n",
      "756\n",
      "757\n",
      "759\n",
      "760\n",
      "764\n",
      "770\n",
      "771\n",
      "773\n",
      "774\n",
      "775\n",
      "778\n",
      "780\n",
      "782\n",
      "783\n",
      "785\n",
      "789\n",
      "790\n",
      "793\n",
      "801\n",
      "812\n",
      "813\n",
      "818\n",
      "821\n",
      "822\n",
      "824\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "835\n",
      "836\n",
      "838\n",
      "839\n",
      "841\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "849\n",
      "850\n",
      "851\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "869\n",
      "871\n",
      "876\n",
      "883\n",
      "884\n",
      "885\n",
      "887\n",
      "891\n",
      "892\n",
      "896\n",
      "899\n",
      "901\n",
      "903\n",
      "909\n",
      "910\n",
      "911\n",
      "913\n",
      "917\n",
      "918\n",
      "921\n",
      "926\n",
      "927\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "939\n",
      "940\n",
      "942\n",
      "943\n",
      "950\n",
      "952\n",
      "953\n",
      "956\n",
      "958\n",
      "961\n",
      "962\n",
      "963\n",
      "976\n",
      "977\n",
      "978\n",
      "980\n",
      "981\n",
      "985\n",
      "987\n",
      "988\n",
      "990\n",
      "991\n",
      "997\n",
      "998\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1007\n",
      "1012\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1021\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1037\n",
      "1039\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1052\n",
      "1053\n",
      "1056\n",
      "1059\n",
      "1060\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1076\n",
      "1077\n",
      "1080\n",
      "1084\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1095\n",
      "1096\n",
      "1101\n",
      "1102\n",
      "1105\n",
      "1109\n",
      "1111\n",
      "1114\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1124\n",
      "1125\n",
      "1132\n",
      "1136\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1149\n",
      "1152\n",
      "1158\n",
      "1160\n",
      "1161\n",
      "1164\n",
      "1166\n",
      "1167\n",
      "1169\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1182\n",
      "1184\n",
      "1187\n",
      "1191\n",
      "1194\n",
      "1198\n",
      "1199\n",
      "1201\n",
      "1202\n",
      "1216\n",
      "1217\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1227\n",
      "1229\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1242\n",
      "1243\n",
      "1245\n",
      "1248\n",
      "1252\n",
      "1253\n",
      "1256\n",
      "1257\n",
      "1259\n",
      "1260\n",
      "1262\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1273\n",
      "1276\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1283\n",
      "1284\n",
      "1286\n",
      "1288\n",
      "1292\n",
      "1298\n",
      "1299\n",
      "1306\n",
      "1308\n",
      "1314\n",
      "1315\n",
      "1317\n",
      "1319\n",
      "1321\n",
      "1322\n",
      "1325\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1334\n",
      "1336\n",
      "1338\n",
      "1343\n",
      "1346\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1352\n",
      "1355\n",
      "1356\n",
      "1358\n",
      "1361\n",
      "1364\n",
      "1367\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1373\n",
      "1374\n",
      "1376\n",
      "1380\n",
      "1382\n",
      "1384\n",
      "1386\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1392\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1405\n",
      "1406\n",
      "1409\n",
      "1412\n",
      "1413\n",
      "1415\n",
      "1418\n",
      "1420\n",
      "1423\n",
      "1424\n",
      "1426\n",
      "1431\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1444\n",
      "1445\n",
      "1450\n",
      "1451\n",
      "1453\n",
      "1454\n",
      "1456\n",
      "1460\n",
      "1464\n",
      "1465\n",
      "1467\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1474\n",
      "1477\n",
      "1482\n",
      "1484\n",
      "1488\n",
      "1489\n",
      "1493\n",
      "1495\n",
      "1496\n",
      "1499\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1518\n",
      "1519\n",
      "1521\n",
      "1522\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1532\n",
      "1536\n",
      "1538\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1545\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1554\n",
      "1556\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1568\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1577\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1594\n",
      "1598\n",
      "1600\n",
      "1603\n",
      "1605\n",
      "1608\n",
      "1613\n",
      "1616\n",
      "1618\n",
      "1620\n",
      "1621\n",
      "1624\n",
      "1626\n",
      "1629\n",
      "1630\n",
      "1632\n",
      "1634\n",
      "1636\n",
      "1637\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1643\n",
      "1644\n",
      "1651\n",
      "1656\n",
      "1657\n",
      "1659\n",
      "1660\n",
      "1664\n",
      "1667\n",
      "1670\n",
      "1674\n",
      "1676\n",
      "1679\n",
      "1682\n",
      "1686\n",
      "1689\n",
      "1691\n",
      "1693\n",
      "1701\n",
      "1705\n",
      "1706\n",
      "1708\n",
      "1709\n",
      "1717\n",
      "1722\n",
      "1724\n",
      "1729\n",
      "1735\n",
      "1737\n",
      "1738\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1748\n",
      "1749\n",
      "1752\n",
      "1754\n",
      "1756\n",
      "1757\n",
      "1760\n",
      "1762\n",
      "1766\n",
      "1769\n",
      "1777\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1787\n",
      "1791\n",
      "1793\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1807\n",
      "1809\n",
      "1813\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1819\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1827\n",
      "1829\n",
      "1834\n",
      "1836\n",
      "1837\n",
      "1839\n",
      "1841\n",
      "1843\n",
      "1844\n",
      "1846\n",
      "1851\n",
      "1857\n",
      "1861\n",
      "1866\n",
      "1873\n",
      "1874\n",
      "1876\n",
      "1878\n",
      "1880\n",
      "1881\n",
      "1883\n",
      "1886\n",
      "1887\n",
      "1891\n",
      "1894\n",
      "1895\n",
      "1897\n",
      "1901\n",
      "1902\n",
      "1905\n",
      "1911\n",
      "1914\n",
      "1918\n",
      "1921\n",
      "1923\n",
      "1925\n",
      "1928\n",
      "1933\n",
      "1938\n",
      "1939\n",
      "1943\n",
      "1947\n",
      "1949\n",
      "1951\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1958\n",
      "1959\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1970\n",
      "1974\n",
      "1979\n",
      "1981\n",
      "1983\n",
      "1987\n",
      "1988\n",
      "1994\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2003\n",
      "2006\n",
      "2007\n",
      "2013\n",
      "2015\n",
      "2017\n",
      "2021\n",
      "2022\n",
      "2026\n",
      "2028\n",
      "2030\n",
      "2032\n",
      "2037\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2053\n",
      "2055\n",
      "2059\n",
      "2062\n",
      "2063\n",
      "2073\n",
      "2074\n",
      "2079\n",
      "2081\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2092\n",
      "2094\n",
      "2095\n",
      "2098\n",
      "2102\n",
      "2107\n",
      "2108\n",
      "2112\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2122\n",
      "2123\n",
      "2126\n",
      "2128\n",
      "2129\n",
      "2131\n",
      "2132\n",
      "2134\n",
      "2135\n",
      "2139\n",
      "2140\n",
      "2144\n",
      "2146\n",
      "2149\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2160\n",
      "2162\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2172\n",
      "2173\n",
      "2175\n",
      "2177\n",
      "2179\n",
      "2180\n",
      "2183\n",
      "2184\n",
      "2186\n",
      "2187\n",
      "2192\n",
      "2193\n",
      "2199\n",
      "2204\n",
      "2205\n",
      "2207\n",
      "2211\n",
      "2212\n",
      "2216\n",
      "2217\n",
      "2220\n",
      "2224\n",
      "2226\n",
      "2227\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2240\n",
      "2245\n",
      "2246\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2257\n",
      "2258\n",
      "2263\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2276\n",
      "2277\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2302\n",
      "2303\n",
      "2306\n",
      "2307\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2320\n",
      "2323\n",
      "2325\n",
      "2328\n",
      "2329\n",
      "2331\n",
      "2332\n",
      "2335\n",
      "2338\n",
      "2340\n",
      "2342\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2353\n",
      "2358\n",
      "2359\n",
      "2361\n",
      "2364\n",
      "2366\n",
      "2370\n",
      "2371\n",
      "2380\n",
      "2381\n",
      "2389\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2399\n",
      "2400\n",
      "2402\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2409\n",
      "2411\n",
      "2416\n",
      "2417\n",
      "2423\n",
      "2425\n",
      "2427\n",
      "2428\n",
      "2434\n",
      "2439\n",
      "2441\n",
      "2445\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2457\n",
      "2460\n",
      "2462\n",
      "2466\n",
      "2469\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2477\n",
      "2478\n",
      "2481\n",
      "2482\n",
      "2484\n",
      "2485\n",
      "2487\n",
      "2489\n",
      "2490\n",
      "2493\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2501\n",
      "2505\n",
      "2507\n",
      "2508\n",
      "2510\n",
      "2511\n",
      "2526\n",
      "2529\n",
      "2536\n",
      "2544\n",
      "2547\n",
      "2549\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2560\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2570\n",
      "2572\n",
      "2576\n",
      "2577\n",
      "2579\n",
      "2581\n",
      "2582\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2612\n",
      "2614\n",
      "2615\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2627\n",
      "2629\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2638\n",
      "2639\n",
      "2646\n",
      "2647\n",
      "2651\n",
      "2653\n",
      "2657\n",
      "2658\n",
      "2660\n",
      "2664\n",
      "2667\n",
      "2669\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2678\n",
      "2679\n",
      "2683\n",
      "2684\n",
      "2687\n",
      "2691\n",
      "2693\n",
      "2695\n",
      "2696\n",
      "2700\n",
      "2702\n",
      "2703\n",
      "2712\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2725\n",
      "2727\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2745\n",
      "2748\n",
      "2750\n",
      "2752\n",
      "2753\n",
      "2756\n",
      "2763\n",
      "2765\n",
      "2766\n",
      "2768\n",
      "2769\n",
      "2771\n",
      "2773\n",
      "2774\n",
      "2776\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2785\n",
      "2787\n",
      "2788\n",
      "2791\n",
      "2801\n",
      "2808\n",
      "2809\n",
      "2813\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2819\n",
      "2822\n",
      "2827\n",
      "2830\n",
      "2831\n",
      "2834\n",
      "2835\n",
      "2840\n",
      "2843\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2851\n",
      "2853\n",
      "2861\n",
      "2862\n",
      "2864\n",
      "2867\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2878\n",
      "2879\n",
      "2881\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2894\n",
      "2903\n",
      "2904\n",
      "2906\n",
      "2910\n",
      "2912\n",
      "2915\n",
      "2918\n",
      "2921\n",
      "2923\n",
      "2925\n",
      "2932\n",
      "2934\n",
      "2936\n",
      "2943\n",
      "2949\n",
      "2951\n",
      "2954\n",
      "2955\n",
      "2957\n",
      "2960\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2976\n",
      "2977\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2988\n",
      "2995\n",
      "2996\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3006\n",
      "3008\n",
      "3010\n",
      "3012\n",
      "3017\n",
      "3021\n",
      "3023\n",
      "3027\n",
      "3028\n",
      "3031\n",
      "3033\n",
      "3034\n",
      "3036\n",
      "3038\n",
      "3039\n",
      "3046\n",
      "3047\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3059\n",
      "3060\n",
      "3064\n",
      "3067\n",
      "3071\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3083\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3094\n",
      "3100\n",
      "3101\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3116\n",
      "3120\n",
      "3121\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3133\n",
      "3138\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3145\n",
      "3147\n",
      "3150\n",
      "3156\n",
      "3161\n",
      "3162\n",
      "3169\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3178\n",
      "3179\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3193\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3205\n",
      "3209\n",
      "3212\n",
      "3214\n",
      "3216\n",
      "3217\n",
      "3219\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3228\n",
      "3229\n",
      "3231\n",
      "3233\n",
      "3235\n",
      "3240\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3247\n",
      "3249\n",
      "3252\n",
      "3253\n",
      "3255\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3268\n",
      "3276\n",
      "3277\n",
      "3279\n",
      "3282\n",
      "3287\n",
      "3291\n",
      "3292\n",
      "3297\n",
      "3298\n",
      "3302\n",
      "3306\n",
      "3307\n",
      "3310\n",
      "3313\n",
      "3316\n",
      "3317\n",
      "3325\n",
      "3332\n",
      "3339\n",
      "3343\n",
      "3345\n",
      "3346\n",
      "3348\n",
      "3355\n",
      "3357\n",
      "3358\n",
      "3362\n",
      "3365\n",
      "3367\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3373\n",
      "3380\n",
      "3385\n",
      "3387\n",
      "3391\n",
      "3395\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3405\n",
      "3408\n",
      "3410\n",
      "3413\n",
      "3417\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3444\n",
      "3446\n",
      "3453\n",
      "3456\n",
      "3459\n",
      "3460\n",
      "3462\n",
      "3466\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3482\n",
      "3484\n",
      "3485\n",
      "3489\n",
      "3491\n",
      "3495\n",
      "3497\n",
      "3502\n",
      "3504\n",
      "3505\n",
      "3508\n",
      "3512\n",
      "3517\n",
      "3518\n",
      "3521\n",
      "3522\n",
      "3529\n",
      "3532\n",
      "3534\n",
      "3538\n",
      "3539\n",
      "3541\n",
      "3542\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3551\n",
      "3557\n",
      "3558\n",
      "3561\n",
      "3562\n",
      "3566\n",
      "3569\n",
      "3570\n",
      "3574\n",
      "3577\n",
      "3582\n",
      "3584\n",
      "3585\n",
      "3587\n",
      "3588\n",
      "3592\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3600\n",
      "3603\n",
      "3605\n",
      "3610\n",
      "3614\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3624\n",
      "3626\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3632\n",
      "3634\n",
      "3636\n",
      "3637\n",
      "3642\n",
      "3644\n",
      "3645\n",
      "3647\n",
      "3648\n",
      "3650\n",
      "3651\n",
      "3654\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3672\n",
      "3674\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3690\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3706\n",
      "3707\n",
      "3714\n",
      "3725\n",
      "3729\n",
      "3731\n",
      "3733\n",
      "3734\n",
      "3736\n",
      "3738\n",
      "3739\n",
      "3741\n",
      "3746\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3755\n",
      "3758\n",
      "3761\n",
      "3765\n",
      "3768\n",
      "3771\n",
      "3773\n",
      "3780\n",
      "3784\n",
      "3785\n",
      "3788\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3797\n",
      "3798\n",
      "3803\n",
      "3807\n",
      "3808\n",
      "3810\n",
      "3814\n",
      "3815\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3829\n",
      "3831\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3839\n",
      "3840\n",
      "3844\n",
      "3851\n",
      "3857\n",
      "3859\n",
      "3860\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3870\n",
      "3873\n",
      "3874\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3886\n",
      "3887\n",
      "3889\n",
      "3893\n",
      "3896\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3908\n",
      "3910\n",
      "3913\n",
      "3915\n",
      "3916\n",
      "3919\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3926\n",
      "3927\n",
      "3931\n",
      "3933\n",
      "3934\n",
      "3940\n",
      "3941\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3953\n",
      "3957\n",
      "3959\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3965\n",
      "3968\n",
      "3970\n",
      "3972\n",
      "3973\n",
      "3979\n",
      "3982\n",
      "3983\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3989\n",
      "3990\n",
      "3992\n",
      "3995\n",
      "3996\n",
      "3998\n",
      "4000\n",
      "4008\n",
      "4009\n",
      "4011\n",
      "4014\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4022\n",
      "4025\n",
      "4035\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4045\n",
      "4051\n",
      "4055\n",
      "4059\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4068\n",
      "4070\n",
      "4074\n",
      "4076\n",
      "4077\n",
      "4079\n",
      "4080\n",
      "4084\n",
      "4088\n",
      "4094\n",
      "4095\n",
      "4100\n",
      "4101\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4112\n",
      "4114\n",
      "4115\n",
      "4117\n",
      "4127\n",
      "4128\n",
      "4135\n",
      "4137\n",
      "4139\n",
      "4140\n",
      "4142\n",
      "4146\n",
      "4147\n",
      "4149\n",
      "4150\n",
      "4154\n",
      "4155\n",
      "4163\n",
      "4170\n",
      "4171\n",
      "4173\n",
      "4175\n",
      "4177\n",
      "4178\n",
      "4180\n",
      "4183\n",
      "4189\n",
      "4194\n",
      "4197\n",
      "4198\n",
      "4204\n",
      "4205\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4213\n",
      "4217\n",
      "4221\n",
      "4222\n",
      "4224\n",
      "4228\n",
      "4229\n",
      "4231\n",
      "4233\n",
      "4239\n",
      "4241\n",
      "4244\n",
      "4247\n",
      "4250\n",
      "4257\n",
      "4258\n",
      "4260\n",
      "4262\n",
      "4266\n",
      "4270\n",
      "4275\n",
      "4276\n",
      "4278\n",
      "4280\n",
      "4287\n",
      "4289\n",
      "4291\n",
      "4292\n",
      "4296\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4308\n",
      "4312\n",
      "4313\n",
      "4315\n",
      "4319\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4325\n",
      "4327\n",
      "4329\n",
      "4330\n",
      "4332\n",
      "4334\n",
      "4336\n",
      "4345\n",
      "4347\n",
      "4350\n",
      "4352\n",
      "4358\n",
      "4359\n",
      "4362\n",
      "4367\n",
      "4368\n",
      "4370\n",
      "4371\n",
      "4373\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4380\n",
      "4381\n",
      "4385\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4409\n",
      "4416\n",
      "4421\n",
      "4422\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4429\n",
      "4432\n",
      "4436\n",
      "4438\n",
      "4444\n",
      "4445\n",
      "4447\n",
      "4450\n",
      "4453\n",
      "4458\n",
      "4463\n",
      "4464\n",
      "4470\n",
      "4471\n",
      "4475\n",
      "4476\n",
      "4478\n",
      "4480\n",
      "4482\n",
      "4484\n",
      "4487\n",
      "4492\n",
      "4498\n",
      "4501\n",
      "4506\n",
      "4508\n",
      "4509\n",
      "4512\n",
      "4520\n",
      "4523\n",
      "4524\n",
      "4528\n",
      "4529\n",
      "4531\n",
      "4536\n",
      "4537\n",
      "4539\n",
      "4542\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4553\n",
      "4557\n",
      "4558\n",
      "4560\n",
      "4562\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4570\n",
      "4572\n",
      "4574\n",
      "4576\n",
      "4581\n",
      "4583\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4592\n",
      "4599\n",
      "4601\n",
      "4604\n",
      "4608\n",
      "4609\n",
      "4612\n",
      "4614\n",
      "4620\n",
      "4621\n",
      "4623\n",
      "4625\n",
      "4630\n",
      "4632\n",
      "4633\n",
      "4639\n",
      "4642\n",
      "4644\n",
      "4648\n",
      "4649\n",
      "4651\n",
      "4652\n",
      "4654\n",
      "4657\n",
      "4663\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4670\n",
      "4672\n",
      "4674\n",
      "4676\n",
      "4677\n",
      "4681\n",
      "4682\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4689\n",
      "4690\n",
      "4692\n",
      "4693\n",
      "4697\n",
      "4698\n",
      "4702\n",
      "4708\n",
      "4711\n",
      "4717\n",
      "4720\n",
      "4725\n",
      "4730\n",
      "4731\n",
      "4733\n",
      "4737\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4746\n",
      "4748\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4756\n",
      "4758\n",
      "4762\n",
      "4764\n",
      "4766\n",
      "4767\n",
      "4769\n",
      "4770\n",
      "4775\n",
      "4776\n",
      "4778\n",
      "4780\n",
      "4781\n",
      "4789\n",
      "4794\n",
      "4796\n",
      "4797\n",
      "4801\n",
      "4807\n",
      "4814\n",
      "4822\n",
      "4826\n",
      "4827\n",
      "4832\n",
      "4836\n",
      "4839\n",
      "4841\n",
      "4843\n",
      "4847\n",
      "4854\n",
      "4855\n",
      "4858\n",
      "4861\n",
      "4862\n",
      "4865\n",
      "4866\n",
      "4868\n",
      "4869\n",
      "4872\n",
      "4874\n",
      "4883\n",
      "4884\n",
      "4886\n",
      "4888\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4897\n",
      "4899\n",
      "4905\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4914\n",
      "4915\n",
      "4917\n",
      "4926\n",
      "4927\n",
      "4929\n",
      "4933\n",
      "4937\n",
      "4938\n",
      "4940\n",
      "4943\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4953\n",
      "4955\n",
      "4956\n",
      "4960\n",
      "4961\n",
      "4963\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4983\n",
      "4987\n",
      "4988\n",
      "4990\n",
      "4994\n",
      "4999\n",
      "5001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for x in range(5000):\n",
    "    if l_out[x] != test_y[x]:\n",
    "        count+=1\n",
    "        print(x+2)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-482d29d34e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"books/books_5.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \"\"\"\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"books/books_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('C:/Users/Adarsh Kumar/Downloads/books_5.json.gz')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "def text_preproc(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = \"\".join(x.split(\"<br />\"))\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"IMDB/train.csv\")\n",
    "valid = pd.read_csv(\"IMDB/valid.csv\")\n",
    "test = pd.read_csv(\"IMDB/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train.text.apply(text_preproc)\n",
    "valid['clean_text'] = valid.text.apply(text_preproc)\n",
    "test['clean_text'] = test.text.apply(text_preproc)\n",
    "train.drop(columns=[\"text\"], inplace= True)\n",
    "valid.drop(columns=[\"text\"], inplace= True)\n",
    "test.drop(columns=[\"text\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "voc_size = 10000\n",
    "onehot_repr = []\n",
    "onehot_repr=onehot_repr+[one_hot(sents,voc_size)for sents in train[\"clean_text\"][:40000]] \n",
    "print(len(onehot_repr))\n",
    "onehot_reprv = []\n",
    "onehot_reprv=onehot_reprv+[one_hot(sents,voc_size)for sents in valid[\"clean_text\"][:5000]] \n",
    "print(len(onehot_reprv))\n",
    "onehot_reprt = []\n",
    "onehot_reprt=onehot_reprt+[one_hot(sents,voc_size)for sents in test[\"clean_text\"][:5000]] \n",
    "print(len(onehot_reprt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 8870 9069 2032]\n",
      " [1827 1134 9948 ... 2432 8919 2574]\n",
      " [4349 4158  144 ... 6506 8161 8389]\n",
      " ...\n",
      " [ 229 1205 2968 ... 5519 5189 3751]\n",
      " [   0    0    0 ... 9729 7306   27]\n",
      " [   0    0    0 ... 6903 6132 8203]]\n",
      "(40000, 100) (40000,)\n",
      "[[1487 2416 2516 ... 8870 5069 9958]\n",
      " [   0    0    0 ... 3793 1514 8393]\n",
      " [   0    0    0 ... 5542 5381 8161]\n",
      " ...\n",
      " [   0    0    0 ... 1478  380 3693]\n",
      " [7268 3711 1741 ... 2905 6629 4299]\n",
      " [   0    0    0 ... 8788 8256 8694]]\n",
      "(5000, 100) (5000,)\n",
      "[[8788 2574 2094 ... 4643 5396 5053]\n",
      " [   0    0 1480 ... 6629 7622 5922]\n",
      " [3793 4354 2714 ... 3793 5135 6052]\n",
      " ...\n",
      " [   0    0    0 ... 1644 3666 3482]\n",
      " [   0    0    0 ... 9268 9268 9268]\n",
      " [   0    0    0 ... 2909 5230 7316]]\n",
      "(5000, 100) (5000,)\n"
     ]
    }
   ],
   "source": [
    "sent_length=100\n",
    "train_x=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "train_y = train.label\n",
    "print(train_x)\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "val_x=pad_sequences(onehot_reprv,padding='pre',maxlen=sent_length)\n",
    "val_y = valid.label\n",
    "print(val_x)\n",
    "print(val_x.shape, val_y.shape)\n",
    "\n",
    "test_x=pad_sequences(onehot_reprt,padding='pre',maxlen=sent_length)\n",
    "test_y = test.label\n",
    "print(test_x)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,366,593\n",
      "Trainable params: 1,366,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,100,input_length=sent_length))\n",
    "model.compile('adam','mse')\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 5000 samples\n",
      "40000/40000 [==============================] - 639s 16ms/sample - loss: 0.4212 - accuracy: 0.8091 - val_loss: 0.3795 - val_accuracy: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167b5c5e2e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=32, \n",
    "          epochs=1,validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 8s 2ms/sample - loss: 0.3805 - accuracy: 0.8342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3805092618703842, 0.8342]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_reprx = []\n",
    "s = \"i can't believe people are looking for a plot in this film\"\n",
    "s = [s]\n",
    "onehot_reprx=onehot_reprx+[one_hot(sents,voc_size)for sents in s] \n",
    "testing_x=pad_sequences(onehot_reprx,padding='pre',maxlen=sent_length)\n",
    "testing_x.shape\n",
    "# model.predict_classes(testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.models.save_model(model,\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11740264]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
